{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPxb0yPBvr0GZU1Mbdf8o1M",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mansueli/mongo-2-postgres/blob/main/mongodump2postgres.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Supabase](https://raw.githubusercontent.com/supabase/supabase/master/packages/common/assets/images/supabase-logo-wordmark--light.svg)\n",
        "\n"
      ],
      "metadata": {
        "id": "mmvqwDq1ANXj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Connection URIs"
      ],
      "metadata": {
        "id": "M_k9R3EIwDYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Source DB variables:\n",
        "%env supabase_uri=postgresql://postgres:<pass>@db.<ref>.supabase.co:5432/postgres\n",
        "%env mongo_uri=mongodb+srv://<user>:<ref>@cluster012.mongodb.net/?retryWrites=true&w=majority"
      ],
      "metadata": {
        "id": "sZnTfGOewGP2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up the environment"
      ],
      "metadata": {
        "id": "bjs07GLrv-ts"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mongo &>log\n",
        "!pip install psycopg2 &>log\n",
        "!sudo sh -c 'echo \"deb http://apt.postgresql.org/pub/repos/apt $(lsb_release -cs)-pgdg main\" > /etc/apt/sources.list.d/pgdg.list'\n",
        "!wget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add -\n",
        "!wget --quiet -O - https://pgp.mongodb.com/server-7.0.asc | sudo gpg -o /usr/share/keyrings/mongodb-server-7.0.gpg --dearmor\n",
        "!echo \"deb [ arch=amd64,arm64 signed-by=/usr/share/keyrings/mongodb-server-7.0.gpg ] https://repo.mongodb.org/apt/ubuntu focal/mongodb-org/7.0 multiverse\" | sudo tee /etc/apt/sources.list.d/mongodb-org-7.0.list\n",
        "!sudo apt-get update &>log\n",
        "!sudo apt -y install postgresql &>log\n",
        "!sudo apt install mongodb-org-tools &>log"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YIEOAJYVsJn3",
        "outputId": "9fdd6721-a125-4c6a-d356-b251af5da160"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: apt-key is deprecated. Manage keyring files in trusted.gpg.d instead (see apt-key(8)).\n",
            "OK\n",
            "deb [ arch=amd64,arm64 signed-by=/usr/share/keyrings/mongodb-server-7.0.gpg ] https://repo.mongodb.org/apt/ubuntu focal/mongodb-org/7.0 multiverse\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating the mongo Backup:"
      ],
      "metadata": {
        "id": "l-8SUX-twCgy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mongodump --uri \"$mongo_uri\" -o ./mongo-backup &>log"
      ],
      "metadata": {
        "id": "kgWDJ6XusxFC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running the migration"
      ],
      "metadata": {
        "id": "RzhAKIyqUXEl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import bson\n",
        "import json\n",
        "import psycopg2\n",
        "from psycopg2 import sql, extensions, connect\n",
        "from datetime import datetime\n",
        "from bson.decimal128 import Decimal128\n",
        "\n",
        "# Define PostgreSQL connection\n",
        "postgres_conn_string = os.environ['supabase_uri']\n",
        "pg_conn = connect(postgres_conn_string)\n",
        "pg_conn.set_isolation_level(extensions.ISOLATION_LEVEL_AUTOCOMMIT)\n",
        "pg_cur = pg_conn.cursor()\n",
        "\n",
        "# Mapping MongoDB types to PostgreSQL types\n",
        "SQL_DATA_TYPE = {\n",
        "  \"str\": \"TEXT\",\n",
        "  \"ObjectId\": \"TEXT\",\n",
        "  \"datetime.datetime\": \"TIMESTAMP WITH TIME ZONE\",\n",
        "  \"datetime\": \"TIMESTAMP WITH TIME ZONE\",\n",
        "  \"int\": \"INT\",\n",
        "  \"list\": \"JSONB\",\n",
        "  \"dict\": \"JSONB\",\n",
        "  \"bool\": \"Boolean\",\n",
        "  \"float\": \"NUMERIC\",\n",
        "  \"default\": \"TEXT\",\n",
        "  \"NoneType\":\"TEXT\",\n",
        "  \"Decimal128\":\"NUMERIC\",\n",
        "}\n",
        "\n",
        "# Define your backup directory here\n",
        "mongo_backup_dir = \"./mongo-backup\"\n",
        "\n",
        "# Function to handle circular references within lists and dictionaries\n",
        "def detect_and_handle_circular_references(obj, seen=None):\n",
        "    if seen is None:\n",
        "        seen = set()\n",
        "    obj_id = id(obj)\n",
        "    if obj_id in seen:\n",
        "        return 'CircularReference'  # Replace circular references with a marker\n",
        "    seen.add(obj_id)\n",
        "    if isinstance(obj, list):\n",
        "        return [detect_and_handle_circular_references(item, seen) for item in obj]\n",
        "    elif isinstance(obj, dict):\n",
        "        return {key: detect_and_handle_circular_references(value, seen) for key, value in obj.items()}\n",
        "    return obj\n",
        "\n",
        "# Function to convert Decimal128 to string within lists and dictionaries\n",
        "def convert_decimal_to_str(obj):\n",
        "    if isinstance(obj, Decimal128):\n",
        "        return str(obj)\n",
        "    return obj\n",
        "\n",
        "# Iterate over all databases in the MongoDB backup directory\n",
        "for database_name in os.listdir(mongo_backup_dir):\n",
        "    # Skip non-directory entries\n",
        "    if not os.path.isdir(os.path.join(mongo_backup_dir, database_name)):\n",
        "        continue\n",
        "\n",
        "    # Iterate over all collections in the current database directory\n",
        "    for collection_file_name in os.listdir(os.path.join(mongo_backup_dir, database_name)):\n",
        "        # Ignore files that do not end in .bson\n",
        "        if not collection_file_name.endswith('.bson'):\n",
        "            continue\n",
        "\n",
        "        collection_name = collection_file_name.replace(\".bson\", \"\")\n",
        "\n",
        "        bson_file_path = os.path.join(mongo_backup_dir, database_name, collection_file_name)\n",
        "        with open(bson_file_path, 'rb') as f:\n",
        "            data = bson.decode_all(f.read())\n",
        "\n",
        "        # Store the type of each field\n",
        "        field_types = {}\n",
        "\n",
        "        # For each document in the collection, infer the schema\n",
        "        for doc in data:\n",
        "            for field, value in doc.items():\n",
        "                # Determine PostgreSQL type based on the Python type\n",
        "                pg_type = SQL_DATA_TYPE.get(str(type(value)), SQL_DATA_TYPE[\"default\"])\n",
        "                field_with_type = f\"{field}\"\n",
        "\n",
        "                if field not in field_types:\n",
        "                    field_types[field] = {field_with_type}\n",
        "                elif field_with_type not in field_types[field]:\n",
        "                    field_types[field].add(field_with_type)\n",
        "\n",
        "        # Construct table name as \"database_collection\"\n",
        "        table_name = f\"{database_name}_{collection_name}\"\n",
        "\n",
        "        # Create table command\n",
        "        pg_cur.execute(\n",
        "            sql.SQL(\"CREATE TABLE IF NOT EXISTS {} ()\").format(\n",
        "                sql.Identifier(table_name)))\n",
        "\n",
        "        # Add new columns to the existing table\n",
        "        for field_with_type_set in field_types.values():\n",
        "            for field_with_type in field_with_type_set:\n",
        "                try:\n",
        "                    pg_cur.execute(sql.SQL(\"ALTER TABLE {} ADD COLUMN IF NOT EXISTS {} {}\").format(\n",
        "                        sql.Identifier(table_name),\n",
        "                        sql.Identifier(field_with_type),\n",
        "                        sql.SQL(SQL_DATA_TYPE[\"default\"]))  # Use the default data type\n",
        "                    )\n",
        "                except psycopg2.errors.DuplicateColumn:\n",
        "                    pass  # Column already exists\n",
        "\n",
        "\n",
        "        # Serialize and save data for each iteration\n",
        "        with open(table_name+'.csv', 'w') as f:\n",
        "            # Write the header line with column names enclosed in double-quotes\n",
        "            header = [f'\"{field_with_type.split(\"_\")[0]}\"' for field_with_type_set in field_types.values() for field_with_type in field_with_type_set]\n",
        "            f.write('\\t'.join(header) + '\\n')\n",
        "\n",
        "            for document in data:\n",
        "                row = []\n",
        "                for field_with_type_set in field_types.values():\n",
        "                    field = next(iter(field_with_type_set)).split(\"_\")[0]  # Get the field name\n",
        "                    for field_with_type in field_with_type_set:\n",
        "                        try:\n",
        "                            value = document.get(field, None)  # Use None if field is not present\n",
        "\n",
        "                            # Continue with the rest of the serialization logic\n",
        "                            if isinstance(value, datetime):  # Serialize datetime objects to string\n",
        "                                value = value.isoformat()\n",
        "                            if value is not None:\n",
        "                                value = str(value)  # Convert None to a string representation\n",
        "                            else:\n",
        "                                value = '\\\\N'  # Use \"\\\\N\" for NULL values\n",
        "                        except Exception as e:\n",
        "                            value_before = f\"Value before serialization: {document.get(field, None)}\"\n",
        "                            value_after = f\"Value after serialization: {value}\"\n",
        "                            print(f\"Error for Field: {field}\")\n",
        "                            print(value_before)\n",
        "                            print(value_after)\n",
        "                            raise e  # Re-raise the exception to halt processing\n",
        "                        row.append(value)\n",
        "                f.write('\\t'.join(row) + '\\n')\n",
        "\n",
        "        # Construct a list of non-empty column names enclosed in double-quotes\n",
        "        non_empty_columns = [f'\"{field}\"' for field in field_types.keys()]\n",
        "\n",
        "        # Execute COPY STDIN command in PSQL with correct column names and double-quotes\n",
        "        pg_conn.close()  # Close the connection before the shell command\n",
        "        column_str = ', '.join(non_empty_columns)\n",
        "        psql_str = f\"psql {postgres_conn_string} -c '\\\\copy public.\\\"{table_name}\\\" FROM {table_name}.csv' WITH CSV HEADER DELIMITER as '\\t'\"\n",
        "        #print(\"!\" + psql_str)\n",
        "        os.system(psql_str)\n",
        "        pg_conn = connect(postgres_conn_string)  # Reopen it afterward\n",
        "        pg_conn.set_isolation_level(extensions.ISOLATION_LEVEL_AUTOCOMMIT)\n",
        "        pg_cur = pg_conn.cursor()\n",
        "\n",
        "# Close PostgreSQL connection\n",
        "pg_cur.close()\n",
        "pg_conn.close()\n"
      ],
      "metadata": {
        "id": "EYKRyuYk0y02"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}